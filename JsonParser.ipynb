{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Json File Dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mahesh KC  \n",
    "Description: Program to dynamically parse JSON dataset. Creates profiling, flatten dataset with column header.  \n",
    "Date: 2020-03-11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove CRLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(string_value):\n",
    "    if string_value == None:\n",
    "        return None\n",
    "    else:\n",
    "        return string_value.replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"\\t\",\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directory HouseKeeping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "file_name = 'style_format.json'\n",
    "incoming_dataset = 'source_dataset'\n",
    "base_dataset = 'processed_dataset'\n",
    "profile_metadata = 'profiled_metadata'\n",
    "processing_dir = file_name.split('.')[0]\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "today = now[0:8]\n",
    "tmp_dir = 'tmp'\n",
    "current_dir = os.path.join(base_dataset, processing_dir,today)\n",
    "profile_dir = os.path.join(profile_metadata, processing_dir, today)\n",
    "tmp_dir = os.path.join(tmp_dir, 'tmp' + '_' + today, processing_dir)\n",
    "\n",
    "# create directory\n",
    "create_dir(incoming_dataset)\n",
    "create_dir(current_dir)\n",
    "create_dir(profile_dir)\n",
    "create_dir(tmp_dir)\n",
    "\n",
    "# output file format\n",
    "file_suffix = 'tsv'\n",
    "field_delimiter = '\\t'\n",
    "record_delimiter = '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move dataset to source directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if your are testing and json file exists in current directory.\n",
    "#import shutil\n",
    "#shutil.move(file_name,incoming_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load JSON File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(incoming_dataset,file_name)) as json_file:\n",
    "    data_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursively parse nested object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_list = list()\n",
    "parent_attr = list()\n",
    "_parent_name = set()\n",
    "column_name_delimiter = '_'\n",
    "def recursive_dict(dict_name):\n",
    "    if isinstance(dict_name, dict):\n",
    "        for index,(k,v) in enumerate(dict_name.items()):\n",
    "            if not isinstance(v, dict) and not isinstance(v, list):\n",
    "                if len(parent_attr) != 0:\n",
    "                    parent_name = column_name_delimiter.join(parent_attr) + column_name_delimiter + k             \n",
    "                else:\n",
    "                    parent_name = k                    \n",
    "                element_tuple = (parent_name,v)\n",
    "                element_list.append(element_tuple)\n",
    "            if isinstance(v, list):\n",
    "                for i in range(len(v)):\n",
    "                    parent_attr.append(k)\n",
    "                    recursive_dict(v[i])\n",
    "                    parent_attr.pop()\n",
    "            if isinstance(v, dict):\n",
    "                parent_attr.append(k)\n",
    "                _parent_name.add(k)\n",
    "                #print(\"Nested dictionary found. Total elements:\",len(v))\n",
    "                recursive_dict(v)\n",
    "                parent_attr.pop()\n",
    "    return element_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create tsv file from original dataset for all nested object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_file = file_name.replace('json','txt')\n",
    "is_dict = isinstance(data_file, dict)\n",
    "dataset = list()\n",
    "if (is_dict):\n",
    "     for k,v in data_file.items():\n",
    "            if isinstance(v, list):\n",
    "                for i in range(len(v)):\n",
    "                    data = recursive_dict(v[i])\n",
    "                    dataset.append(data[:])\n",
    "                    # Writing to file is consuming lot of memory so this is disabled\n",
    "                    #with open(os.path.join(tmp_dir, dataset_file), 'a+') as datafile:\n",
    "                        #datafile.writelines(str(data) + \"\\n\")\n",
    "                    #for tup in range(len(data)):\n",
    "                        #with open(os.path.join(tmp_dir,'flattened_data.tsv'), 'a+') as realdata:\n",
    "                            #data_value = clean_string(data[tup][1])\n",
    "                            #realdata.writelines(data_value + '\\t')\n",
    "                            #if tup == len(data) - 1:\n",
    "                                #realdata.writelines(\"\\n\")\n",
    "                    element_list.clear()\n",
    "            else:\n",
    "                data = recursive_dict(data_file)\n",
    "                dataset.append(data[:])\n",
    "                element_list.clear()\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data profiling to store metadata about file and all the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import getpass\n",
    "import hashlib\n",
    "profile_report_name = file_name.split('.')[0] + now + '_profiling.' + file_name.split('.')[1]\n",
    "def profile_dataset(data_element):\n",
    "    data_element_profile = OrderedDict()\n",
    "    metadata_data = OrderedDict()\n",
    "    attribute_data = OrderedDict()\n",
    "    dataset_properties = list()\n",
    "    metadata_type = ['created_by','created_date', 'email', 'total_data_element']\n",
    "    attribute_type = ['record_id', 'total_element', 'attribute_name', 'attribute_hash']\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    email_address = 'engineering@analyticstensor.com'\n",
    "    metadata_data[metadata_type[0]] = getpass.getuser()\n",
    "    metadata_data[metadata_type[1]] = datetime.now().strftime(date_format)\n",
    "    metadata_data[metadata_type[2]] = email_address\n",
    "    metadata_data[metadata_type[3]] = len(data_element)\n",
    "    data_element_profile['metadata'] = metadata_data\n",
    "    for i in range(len(data_element)):\n",
    "        attribute_name = ''        \n",
    "        for j in range(len(data_element[i])):\n",
    "            attribute_name += data_element[i][j][0] + ',' \n",
    "        attribute_data[attribute_type[0]] = i + 1\n",
    "        attribute_data[attribute_type[1]] = len(data_element[i])\n",
    "        attribute_data[attribute_type[2]] = attribute_name[0:len(attribute_name)-1]\n",
    "        attribute_data[attribute_type[3]] = hashlib.sha1(attribute_data[attribute_type[2]].lower().encode()).hexdigest()\n",
    "        dataset_properties.append(attribute_data.copy())\n",
    "        data_element_profile['data_attributes'] = dataset_properties\n",
    "    profile_report = json.dumps(data_element_profile, separators=(',', ': '), indent = 4)\n",
    "    with open(os.path.join(profile_dir, profile_report_name), 'a+') as profiling:\n",
    "        profiling.writelines(profile_report)\n",
    "    print(\"Data profiling report for {} is located at {} directory with filename {}\".format(file_name,profile_dir,profile_report_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data profiling report for style_format.json is located at profiled_metadata/style_format/20210329 directory with filename style_format20210329180502_profiling.json\n"
     ]
    }
   ],
   "source": [
    "profile_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, 0)</td>\n",
       "      <td>(detail_info_country_name, Global Country)</td>\n",
       "      <td>(detail_info_country_capital, None)</td>\n",
       "      <td>(detail_info_popular_state, None)</td>\n",
       "      <td>(detail_info_country_phone_code, 00)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_vowel, 100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, 1)</td>\n",
       "      <td>(detail_info_country_name, United States)</td>\n",
       "      <td>(detail_info_country_capital, Washing DC)</td>\n",
       "      <td>(detail_info_country_phone_code, 01)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_constant, 80)</td>\n",
       "      <td>(detail_info_style_vowel, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, 2)</td>\n",
       "      <td>(detail_info_country_name, European)</td>\n",
       "      <td>(detail_info_country_capital, Washing DC)</td>\n",
       "      <td>(detail_info_country_phone_code, 00)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_constant, 100)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, 3)</td>\n",
       "      <td>(detail_info_country_name, United Kingdom)</td>\n",
       "      <td>(detail_info_country_capital, Washing DC)</td>\n",
       "      <td>(detail_info_country_phone_code, 00)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_constant, 80)</td>\n",
       "      <td>(detail_info_style_vowel, 20)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                           1  \\\n",
       "0  (id, 0)  (detail_info_country_name, Global Country)   \n",
       "1  (id, 1)   (detail_info_country_name, United States)   \n",
       "2  (id, 2)        (detail_info_country_name, European)   \n",
       "3  (id, 3)  (detail_info_country_name, United Kingdom)   \n",
       "\n",
       "                                           2  \\\n",
       "0        (detail_info_country_capital, None)   \n",
       "1  (detail_info_country_capital, Washing DC)   \n",
       "2  (detail_info_country_capital, Washing DC)   \n",
       "3  (detail_info_country_capital, Washing DC)   \n",
       "\n",
       "                                      3                                     4  \\\n",
       "0     (detail_info_popular_state, None)  (detail_info_country_phone_code, 00)   \n",
       "1  (detail_info_country_phone_code, 01)        (detail_info_style_version, 1)   \n",
       "2  (detail_info_country_phone_code, 00)        (detail_info_style_version, 1)   \n",
       "3  (detail_info_country_phone_code, 00)        (detail_info_style_version, 1)   \n",
       "\n",
       "                                             5  \\\n",
       "0               (detail_info_style_version, 1)   \n",
       "1  (detail_info_style_create_date, 2019-10-04)   \n",
       "2  (detail_info_style_create_date, 2019-10-04)   \n",
       "3  (detail_info_style_create_date, 2019-10-04)   \n",
       "\n",
       "                                             6                               7  \n",
       "0  (detail_info_style_create_date, 2019-10-04)  (detail_info_style_vowel, 100)  \n",
       "1             (detail_info_style_constant, 80)   (detail_info_style_vowel, 20)  \n",
       "2            (detail_info_style_constant, 100)                            None  \n",
       "3             (detail_info_style_constant, 80)   (detail_info_style_vowel, 20)  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)   # set pd columns option\n",
    "\n",
    "df_data = pd.DataFrame.from_records(dataset)\n",
    "df_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorize and collect records having same hashcode and add header**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(os.path.join(profile_dir, profile_report_name)))\n",
    "df_metadata = pd.DataFrame(data[\"data_attributes\"])   # load data_attributes objects\n",
    "all_records_attributes = df_metadata['attribute_hash']\n",
    "\n",
    "# Add record_id and attribute_hash in original dataset\n",
    "df_data['record_id'] = df_metadata['record_id']\n",
    "df_data['attribute_hash'] = df_metadata['attribute_hash']\n",
    "df_data['attribute_name'] = df_metadata['attribute_name']\n",
    "\n",
    "# create a dictionary for grouped records having same attribute_hash key\n",
    "df_grouped_record = {}\n",
    "for i in range(len(df_data)):\n",
    "    df_attr = all_records_attributes[i]\n",
    "    # add dataframe to dictionary\n",
    "    df_tmp = df_data[df_data['attribute_hash'] == df_attr].dropna(axis=1)  # filter and remove empty columns\n",
    "    column_header = df_data['attribute_name'].tolist()[i] + \",record_id,attribute_hash\"\n",
    "    column_header = str(column_header)[:len(str(column_header))].split(',') \n",
    "    df_tmp.drop(columns=['attribute_name'], inplace=True)\n",
    "    df_tmp.columns = column_header\n",
    "    df_grouped_record[df_attr] = df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processed Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>detail_info_country_name</th>\n",
       "      <th>detail_info_country_capital</th>\n",
       "      <th>detail_info_country_phone_code</th>\n",
       "      <th>detail_info_style_version</th>\n",
       "      <th>detail_info_style_create_date</th>\n",
       "      <th>detail_info_style_constant</th>\n",
       "      <th>detail_info_style_vowel</th>\n",
       "      <th>record_id</th>\n",
       "      <th>attribute_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, 1)</td>\n",
       "      <td>(detail_info_country_name, United States)</td>\n",
       "      <td>(detail_info_country_capital, Washing DC)</td>\n",
       "      <td>(detail_info_country_phone_code, 01)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_constant, 80)</td>\n",
       "      <td>(detail_info_style_vowel, 20)</td>\n",
       "      <td>2</td>\n",
       "      <td>58e25cfcd4f1556adae0ab45fd228e558f85a189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, 3)</td>\n",
       "      <td>(detail_info_country_name, United Kingdom)</td>\n",
       "      <td>(detail_info_country_capital, Washing DC)</td>\n",
       "      <td>(detail_info_country_phone_code, 00)</td>\n",
       "      <td>(detail_info_style_version, 1)</td>\n",
       "      <td>(detail_info_style_create_date, 2019-10-04)</td>\n",
       "      <td>(detail_info_style_constant, 80)</td>\n",
       "      <td>(detail_info_style_vowel, 20)</td>\n",
       "      <td>4</td>\n",
       "      <td>58e25cfcd4f1556adae0ab45fd228e558f85a189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                    detail_info_country_name  \\\n",
       "1  (id, 1)   (detail_info_country_name, United States)   \n",
       "3  (id, 3)  (detail_info_country_name, United Kingdom)   \n",
       "\n",
       "                 detail_info_country_capital  \\\n",
       "1  (detail_info_country_capital, Washing DC)   \n",
       "3  (detail_info_country_capital, Washing DC)   \n",
       "\n",
       "         detail_info_country_phone_code       detail_info_style_version  \\\n",
       "1  (detail_info_country_phone_code, 01)  (detail_info_style_version, 1)   \n",
       "3  (detail_info_country_phone_code, 00)  (detail_info_style_version, 1)   \n",
       "\n",
       "                 detail_info_style_create_date  \\\n",
       "1  (detail_info_style_create_date, 2019-10-04)   \n",
       "3  (detail_info_style_create_date, 2019-10-04)   \n",
       "\n",
       "         detail_info_style_constant        detail_info_style_vowel  record_id  \\\n",
       "1  (detail_info_style_constant, 80)  (detail_info_style_vowel, 20)          2   \n",
       "3  (detail_info_style_constant, 80)  (detail_info_style_vowel, 20)          4   \n",
       "\n",
       "                             attribute_hash  \n",
       "1  58e25cfcd4f1556adae0ab45fd228e558f85a189  \n",
       "3  58e25cfcd4f1556adae0ab45fd228e558f85a189  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flatten tuples to retrieve values and store in file for each dictionary keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,v in df_grouped_record.items():\n",
    "    filename = os.path.join(tmp_dir, k + \"_\" + now + \".\" + file_suffix)\n",
    "    file = open(filename, \"w\")\n",
    "    cols = df_grouped_record[k].columns.tolist()\n",
    "    clean_col = [c.strip() for c in cols]\n",
    "    clean_col = str(clean_col).replace(\"'\",\"\").replace(\", \",field_delimiter).replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    file.write(clean_col)\n",
    "    file.write(record_delimiter)\n",
    "    for rec in df_grouped_record[k].itertuples(index=False):   # iteritems()\n",
    "        record = list(rec)\n",
    "        for i in range(len(record)):\n",
    "            if i >=len(record)-2:\n",
    "                value = str(record[i]) + field_delimiter\n",
    "            else:\n",
    "                value = clean_string(str(record[i][1])) + field_delimiter\n",
    "            file.write(value)\n",
    "        file.write(record_delimiter)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Glob multiple files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "file_path = os.path.join(tmp_dir, '*.tsv')\n",
    "files = glob.glob(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read all file in pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dfAll = pd.concat(map(lambda file: pd.read_csv(file, sep='\\t', header='infer', index_col=False), files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_hash</th>\n",
       "      <th>detail_info_country_capital</th>\n",
       "      <th>detail_info_country_name</th>\n",
       "      <th>detail_info_country_phone_code</th>\n",
       "      <th>detail_info_popular_state</th>\n",
       "      <th>detail_info_style_constant</th>\n",
       "      <th>detail_info_style_create_date</th>\n",
       "      <th>detail_info_style_version</th>\n",
       "      <th>detail_info_style_vowel</th>\n",
       "      <th>id</th>\n",
       "      <th>record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58e25cfcd4f1556adae0ab45fd228e558f85a189</td>\n",
       "      <td>Washing DC</td>\n",
       "      <td>United States</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58e25cfcd4f1556adae0ab45fd228e558f85a189</td>\n",
       "      <td>Washing DC</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60fc94cea6d14a741d32683574a73fbba81e7513</td>\n",
       "      <td>Washing DC</td>\n",
       "      <td>European</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2019-10-04</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             attribute_hash detail_info_country_capital  \\\n",
       "0  58e25cfcd4f1556adae0ab45fd228e558f85a189                  Washing DC   \n",
       "1  58e25cfcd4f1556adae0ab45fd228e558f85a189                  Washing DC   \n",
       "2  60fc94cea6d14a741d32683574a73fbba81e7513                  Washing DC   \n",
       "\n",
       "  detail_info_country_name  detail_info_country_phone_code  \\\n",
       "0            United States                               1   \n",
       "1           United Kingdom                               0   \n",
       "2                 European                               0   \n",
       "\n",
       "  detail_info_popular_state  detail_info_style_constant  \\\n",
       "0                       NaN                        80.0   \n",
       "1                       NaN                        80.0   \n",
       "2                       NaN                       100.0   \n",
       "\n",
       "  detail_info_style_create_date  detail_info_style_version  \\\n",
       "0                    2019-10-04                          1   \n",
       "1                    2019-10-04                          1   \n",
       "2                    2019-10-04                          1   \n",
       "\n",
       "   detail_info_style_vowel  id  record_id  \n",
       "0                     20.0   1          2  \n",
       "1                     20.0   3          4  \n",
       "2                      NaN   2          3  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort columns order based on highest columns attribute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get highest attribute_hash value\n",
    "max_attribute_hash = df_metadata[df_metadata['total_element'] == df_metadata['total_element'].max()]['attribute_hash'].values[0]\n",
    "ordered_column = df_metadata[df_metadata['attribute_hash'] == max_attribute_hash]['attribute_name'].values[0].split(\",\")\n",
    "ordered_column += ['record_id', 'attribute_hash']\n",
    "dfAll = dfAll[ordered_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop empty parent column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = [col for col in dfAll.columns.tolist() if col in _parent_name]\n",
    "dfAll = dfAll.drop(columns=remove_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Final Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort order by record_id asc\n",
    "dfAll = dfAll.sort_values(by=['record_id'])\n",
    "dfAll.to_csv(os.path.join(current_dir,\"allfile.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean tmp directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**@todo:**  \n",
    "* Sample all the records.  \n",
    "* Determine column data type check for elastic search usecases.     \n",
    "* Generate summary of the report.  \n",
    "* Add logging for all the transactions.   \n",
    "\n",
    "  **Done**\n",
    "    * Optimization (Exclude write tmp files to reduce processing time) done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>detail_info_country_phone_code</th>\n",
       "      <th>detail_info_style_version</th>\n",
       "      <th>detail_info_style_vowel</th>\n",
       "      <th>record_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.290994</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.188022</td>\n",
       "      <td>1.290994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  detail_info_country_phone_code  detail_info_style_version  \\\n",
       "count  4.000000                            4.00                        4.0   \n",
       "mean   1.500000                            0.25                        1.0   \n",
       "std    1.290994                            0.50                        0.0   \n",
       "min    0.000000                            0.00                        1.0   \n",
       "25%    0.750000                            0.00                        1.0   \n",
       "50%    1.500000                            0.00                        1.0   \n",
       "75%    2.250000                            0.25                        1.0   \n",
       "max    3.000000                            1.00                        1.0   \n",
       "\n",
       "       detail_info_style_vowel  record_id  \n",
       "count                 3.000000   4.000000  \n",
       "mean                 46.666667   2.500000  \n",
       "std                  46.188022   1.290994  \n",
       "min                  20.000000   1.000000  \n",
       "25%                  20.000000   1.750000  \n",
       "50%                  20.000000   2.500000  \n",
       "75%                  60.000000   3.250000  \n",
       "max                 100.000000   4.000000  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfAll.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
